{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from birdCLEFloaddata import load_audiofile,load_metadata,get_melspectrogram\n",
    "from birdCLEFFunctions import Dynamic_CNN, Dynamic_CNN2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import Resize\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torchvision.transforms import Grayscale, ToPILImage\n",
    "import torchvision\n",
    "import soundfile as sf\n",
    "import random\n",
    "import math\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "path = r\"C:\\Users\\zhakk\\Desktop\\Uni\\Kandidat\\AML-BirdCLEFproject\\data\\birdCLEF2023\"\n",
    "\n",
    "meta_data = load_metadata(path,trim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red-backed Scrub-Robin              50\n",
      "Green-backed Camaroptera            50\n",
      "Variable Sunbird                    50\n",
      "Lawrence's Goldfinch                50\n",
      "African Paradise-Flycatcher         50\n",
      "Black Kite                          50\n",
      "White-browed Coucal                 50\n",
      "Pied Kingfisher                     50\n",
      "Tambourine Dove                     50\n",
      "Blue-cheeked Bee-eater              50\n",
      "Ring-necked Dove                    50\n",
      "Hadada Ibis                         50\n",
      "Little Egret                        50\n",
      "Helmeted Guineafowl                 50\n",
      "Red-fronted Tinkerbird              50\n",
      "Barn Swallow                        50\n",
      "Spotted Morning-Thrush              50\n",
      "Blacksmith Lapwing                  50\n",
      "Gray-headed Kingfisher              50\n",
      "Striated Heron                      50\n",
      "Chinspot Batis                      50\n",
      "Cinnamon Bracken-Warbler            50\n",
      "Willow Warbler                      50\n",
      "Yellow-rumped Tinkerbird            50\n",
      "Red-rumped Swallow                  50\n",
      "Slate-colored Boubou                50\n",
      "Green Woodhoopoe                    50\n",
      "Sombre Greenbul                     50\n",
      "Black Cuckoo                        50\n",
      "Gray-backed Camaroptera             50\n",
      "Brubru                              50\n",
      "White-browed Robin-Chat             50\n",
      "Great Cormorant                     50\n",
      "Black-crowned Tchagra               50\n",
      "Gray-capped Warbler                 50\n",
      "Little Swift                        50\n",
      "Collared Sunbird                    50\n",
      "White-browed Sparrow-Weaver         50\n",
      "Scarlet-chested Sunbird             50\n",
      "African Pied Wagtail                50\n",
      "Tawny-flanked Prinia                50\n",
      "Golden-breasted Bunting             50\n",
      "Pin-tailed Whydah                   50\n",
      "Gray-headed Bushshrike              50\n",
      "Red-chested Cuckoo                  50\n",
      "Common Sandpiper                    50\n",
      "Cattle Egret                        50\n",
      "Garganey                            50\n",
      "Tropical Boubou                     50\n",
      "Sulphur-breasted Bushshrike         50\n",
      "Egyptian Goose                      50\n",
      "Great Egret                         50\n",
      "Fork-tailed Drongo                  50\n",
      "African Goshawk                     50\n",
      "Eurasian Hoopoe                     50\n",
      "Common House-Martin                 50\n",
      "Wood Sandpiper                      50\n",
      "African Black-headed Oriole         50\n",
      "Black Crake                         50\n",
      "Yellow-fronted Canary               50\n",
      "Brown-crowned Tchagra               50\n",
      "Pied Crow                           50\n",
      "Western Yellow Wagtail              50\n",
      "Klaas's Cuckoo                      50\n",
      "Black-backed Puffback               50\n",
      "Yellow-whiskered Greenbul           50\n",
      "Red-eyed Dove                       50\n",
      "Woodland Kingfisher                 50\n",
      "Thrush Nightingale                  50\n",
      "Laughing Dove                       50\n",
      "Brown-throated Wattle-eye           50\n",
      "African Gray Hornbill               50\n",
      "Common Buzzard                      50\n",
      "Red-cheeked Cordonbleu              50\n",
      "Singing Cisticola                   50\n",
      "Common Bulbul                       50\n",
      "Village Weaver                      50\n",
      "African Emerald Cuckoo              50\n",
      "Yellow-breasted Apalis              50\n",
      "Rattling Cisticola                  50\n",
      "Emerald-spotted Wood-Dove           50\n",
      "European Bee-eater                  50\n",
      "Cape Robin-Chat                     50\n",
      "Dideric Cuckoo                      50\n",
      "Spur-winged Lapwing                 50\n",
      "Crowned Hornbill                    49\n",
      "Squacco Heron                       48\n",
      "White-fronted Bee-eater             48\n",
      "African Fish-Eagle                  48\n",
      "Red-billed Firefinch                47\n",
      "Black-and-white-casqued Hornbill    47\n",
      "Northern Gray-headed Sparrow        46\n",
      "African Thrush                      45\n",
      "Silvery-cheeked Hornbill            45\n",
      "Greater Blue-eared Starling         45\n",
      "Quailfinch                          44\n",
      "Cardinal Woodpecker                 43\n",
      "Amethyst Sunbird                    43\n",
      "Red-faced Crombec                   42\n",
      "Southern Fiscal                     42\n",
      "Speckled Mousebird                  41\n",
      "Lesser Striped Swallow              40\n",
      "Brown Woodland-Warbler              40\n",
      "Mariqua Sunbird                     40\n",
      "Bronze Mannikin                     38\n",
      "Black-fronted Bushshrike            38\n",
      "Mourning Collared-Dove              37\n",
      "Speckled Pigeon                     37\n",
      "Superb Starling                     37\n",
      "Crowned Eagle                       36\n",
      "Mocking Cliff-Chat                  36\n",
      "Yellow-bellied Eremomela            35\n",
      "Cabanis's Greenbul                  34\n",
      "Yellow-billed Barbet                34\n",
      "Kenya Rufous Sparrow                34\n",
      "Waller's Starling                   34\n",
      "Gabar Goshawk                       34\n",
      "Pale White-eye                      34\n",
      "Beautiful Sunbird                   34\n",
      "Rüppell's Starling                  33\n",
      "Slender-tailed Nightjar             33\n",
      "Streaky Seedeater                   33\n",
      "Meyer's Parrot                      32\n",
      "D'Arnaud's Barbet                   32\n",
      "Spectacled Weaver                   32\n",
      "African Dusky Flycatcher            31\n",
      "Hamerkop                            30\n",
      "Bronze Sunbird                      30\n",
      "African Jacana                      30\n",
      "Snowy-crowned Robin-Chat            30\n",
      "Buff-throated Apalis                30\n",
      "Mountain Wagtail                    30\n",
      "Chubb's Cisticola                   29\n",
      "Brimstone Canary                    29\n",
      "Yellow-spotted Barbet               29\n",
      "Little Bee-eater                    29\n",
      "Black-faced Rufous-Warbler          28\n",
      "Red-winged Starling                 28\n",
      "Abyssinian Thrush                   28\n",
      "White-throated Bee-eater            28\n",
      "Northern Red-billed Hornbill        28\n",
      "Gray Apalis                         28\n",
      "Yellow-throated Greenbul            27\n",
      "Blue-spotted Wood-Dove              27\n",
      "Chestnut-throated Apalis            27\n",
      "White-bellied Go-away-bird          27\n",
      "Blue-naped Mousebird                26\n",
      "Green-winged Pytilia                26\n",
      "Reichenow's Seedeater               25\n",
      "Scaly-throated Honeyguide           25\n",
      "African Green-Pigeon                25\n",
      "Fan-tailed Raven                    25\n",
      "Gray-headed Nigrita                 24\n",
      "Northern Double-collared Sunbird    24\n",
      "Baglafecht Weaver                   24\n",
      "Slender-billed Greenbul             23\n",
      "White-bellied Tit                   23\n",
      "Black-headed Gonolek                23\n",
      "Rock Martin                         22\n",
      "Brown Babbler                       22\n",
      "Black-collared Apalis               22\n",
      "Somali Tit                          22\n",
      "Nubian Woodpecker                   22\n",
      "Yellow-bellied Greenbul             22\n",
      "Eastern Double-collared Sunbird     21\n",
      "Southern Citril                     21\n",
      "Southern Black-Flycatcher           21\n",
      "Red-billed Oxpecker                 21\n",
      "Northern Crombec                    21\n",
      "Northern Fiscal                     20\n",
      "Gray Wren-Warbler                   20\n",
      "Northern Brownbul                   20\n",
      "Black-tailed Oriole                 20\n",
      "Northern Puffback                   20\n",
      "Yellow Bishop                       19\n",
      "Violet-backed Starling              19\n",
      "White-headed Buffalo-Weaver         19\n",
      "Green-headed Sunbird                19\n",
      "Lühder's Bushshrike                 19\n",
      "Little Weaver                       18\n",
      "African Blue Flycatcher             18\n",
      "Vitelline Masked-Weaver             18\n",
      "Hartlaub's Turaco                   18\n",
      "White-eyed Slaty-Flycatcher         17\n",
      "Yellow-billed Duck                  17\n",
      "Red-and-yellow Barbet               17\n",
      "Black-necked Weaver                 17\n",
      "Hunter's Cisticola                  16\n",
      "Northern Black-Flycatcher           16\n",
      "Madagascar Bee-eater                16\n",
      "Black-headed Heron                  16\n",
      "African Bare-eyed Thrush            15\n",
      "Fischer's Lovebird                  15\n",
      "Eastern Mountain Greenbul           15\n",
      "Black-winged Lapwing                15\n",
      "Yellow-spotted Bush Sparrow         15\n",
      "Long-crested Eagle                  15\n",
      "Malachite Kingfisher                15\n",
      "Lesser Masked-Weaver                14\n",
      "Pale Flycatcher                     14\n",
      "White Helmetshrike                  14\n",
      "Pygmy Batis                         14\n",
      "Black Sawwing                       13\n",
      "Cinnamon-chested Bee-eater          13\n",
      "Yellow-crowned Canary               13\n",
      "Spot-flanked Barbet                 13\n",
      "Gray Crowned-Crane                  12\n",
      "Augur Buzzard                       12\n",
      "Speckle-fronted Weaver              12\n",
      "Red-fronted Barbet                  12\n",
      "Yellow-necked Francolin             12\n",
      "Black-throated Apalis               11\n",
      "White-bellied Canary                10\n",
      "Brown-chested Alethe                10\n",
      "Ross's Turaco                       10\n",
      "Purple Grenadier                    10\n",
      "Gray-backed Fiscal                  10\n",
      "Buff-bellied Warbler                 9\n",
      "White-chinned Prinia                 9\n",
      "Eastern Violet-backed Sunbird        9\n",
      "Red-fronted Prinia                   9\n",
      "Gray-throated Barbet                 9\n",
      "Dusky Turtle-Dove                    8\n",
      "Hinde's Pied-Babbler                 8\n",
      "African Gray Flycatcher              8\n",
      "White-rumped Shrike                  8\n",
      "Kikuyu White-eye                     8\n",
      "Chestnut Weaver                      8\n",
      "Mouse-colored Penduline-Tit          8\n",
      "Black-and-white Mannikin             7\n",
      "Equatorial Akalat                    7\n",
      "Black-throated Barbet                7\n",
      "Joyful Greenbul                      7\n",
      "Parrot-billed Sparrow                7\n",
      "White-browed Crombec                 7\n",
      "African Darter                       7\n",
      "Tacazze Sunbird                      6\n",
      "Rufous Chatterer                     6\n",
      "Marabou Stork                        6\n",
      "African Sacred Ibis                  6\n",
      "Mackinnon's Shrike                   6\n",
      "Chestnut Sparrow                     6\n",
      "Pale Prinia                          6\n",
      "Fan-tailed Widowbird                 5\n",
      "Red-headed Weaver                    5\n",
      "Hunter's Sunbird                     5\n",
      "Double-toothed Barbet                5\n",
      "Wire-tailed Swallow                  5\n",
      "Brown-capped Weaver                  5\n",
      "Brown-tailed Chat                    5\n",
      "Golden-backed Weaver                 5\n",
      "Bristle-crowned Starling             5\n",
      "Shelley's Starling                   5\n",
      "Long-toed Lapwing                    5\n",
      "Stuhlmann's Starling                 5\n",
      "Golden-breasted Starling             4\n",
      "Red-headed Bluebill                  3\n",
      "White-crested Turaco                 3\n",
      "Long-tailed Cormorant                3\n",
      "Goliath Heron                        2\n",
      "Yellow-billed Stork                  2\n",
      "Crested Francolin                    1\n",
      "African Pygmy Kingfisher             1\n",
      "White-headed Sawwing                 1\n",
      "Name: common_name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "print(meta_data['common_name'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audiofile2(filepath, cutoff_time):\n",
    "    #Function that takes a file and cutoff time to create training and validation sets for training\n",
    "    #If the audioclip is lower than the cutoff_time then the clip is looped untill desired duration reached\n",
    "    audio, sr = sf.read(filepath)\n",
    "    duration = len(audio) / sr\n",
    "\n",
    "\n",
    "    if duration >= cutoff_time:\n",
    "        training_audio = audio[:int(sr * 15)]\n",
    "        validation_audio = audio[int(sr * 15):int(sr * 30)]\n",
    "    else:\n",
    "        \n",
    "        loop_count = int(np.ceil(cutoff_time / duration))\n",
    "        audio = np.tile(audio, loop_count)\n",
    "\n",
    "        training_audio = audio[:int(sr * 15)]\n",
    "        validation_audio = audio[int(sr * 15):int(sr * 30)]\n",
    "\n",
    "    return training_audio.astype(np.float32), validation_audio.astype(np.float32), sr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First/simple data extraction methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate audio data\n",
    "training_data = []\n",
    "validation_data = []\n",
    "for i in range(50):\n",
    "    train_audio,validation_audio, sr = load_audiofile2(meta_data['filename'][i],30)\n",
    "    training_data.append([get_melspectrogram(train_audio),meta_data['common_name'][i]])\n",
    "    validation_data.append([get_melspectrogram(validation_audio),meta_data['common_name'][i]])\n",
    "\n",
    "\n",
    "training_data = np.asarray(training_data)\n",
    "validation_data = np.asarray(validation_data)\n",
    "label_mapping = {label: index for index, label in enumerate(set(meta_data['common_name'].unique()))}\n",
    "training_data[:,1] = [label_mapping.get(label, -1)+1 for label in training_data[:,1]]\n",
    "validation_data[:,1] = [label_mapping.get(label, -1)+1 for label in validation_data[:,1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Generate audio data\n",
    "training_data = []\n",
    "validation_data = []\n",
    "birds_with_single_clip = []\n",
    "\n",
    "# Iterate through each unique bird\n",
    "for bird in meta_data['common_name'].unique():\n",
    "    print(bird)\n",
    "    # Get all audio clips for the bird\n",
    "    bird_clips = meta_data[meta_data['common_name'] == bird]['filename'].tolist()\n",
    "    # If the bird has only one clip, split it into two halves\n",
    "    if len(bird_clips) == 1:\n",
    "        birds_with_single_clip.append(bird)\n",
    "    else:\n",
    "        # Randomly select one clip for validation and the rest for training\n",
    "        random.shuffle(bird_clips)\n",
    "        training_clip = bird_clips[1:]\n",
    "        validation_clip = bird_clips[0]\n",
    "    \n",
    "    # Load and process the training clip\n",
    "    for clip in training_clip:\n",
    "        train_audio, sr = load_audiofile(clip)\n",
    "        training_data.append([get_melspectrogram(train_audio), bird])\n",
    "    \n",
    "    # Load and process the validation clip\n",
    "    validation_audio, sr = load_audiofile(validation_clip)\n",
    "    validation_data.append([get_melspectrogram(validation_audio), bird])\n",
    "\n",
    "# Split the single clips into training and validation\n",
    "random.shuffle(birds_with_single_clip)\n",
    "split_index = len(birds_with_single_clip) // 2\n",
    "training_single_clips = birds_with_single_clip[split_index:]\n",
    "validation_single_clips = birds_with_single_clip[:split_index]\n",
    "\n",
    "# Append the single clips to the training and validation data\n",
    "for bird in birds_with_single_clip:\n",
    "    clip = meta_data[meta_data['common_name'] == bird]['filename'].tolist()[0]\n",
    "    audio, sr = load_audiofile(clip)\n",
    "    split_index = len(audio) // 2\n",
    "    val_audio = audio[:split_index]\n",
    "    train_audio = audio[split_index:]\n",
    "    validation_data.append([get_melspectrogram(val_audio), bird])\n",
    "    training_data.append([get_melspectrogram(train_audio), bird])\n",
    "\n",
    "\n",
    "# Convert to numpy arrays\n",
    "training_data = np.asarray(training_data)\n",
    "validation_data = np.asarray(validation_data)\n",
    "\n",
    "# Map labels to indices\n",
    "label_mapping = {label: index for index, label in enumerate(set(meta_data['common_name'].unique()))}\n",
    "training_data[:, 1] = [label_mapping.get(label, -1) + 1 for label in training_data[:, 1]]\n",
    "validation_data[:, 1] = [label_mapping.get(label, -1) + 1 for label in validation_data[:, 1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_audio.ndim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#Generate audio data\n",
    "training_data = []\n",
    "validation_data = []\n",
    "cutoff_time = 30\n",
    "\n",
    "for common_name in meta_data['common_name'].unique():\n",
    "    i = meta_data.loc[meta_data['common_name'] == common_name].index[0] \n",
    "    train_audio,validation_audio, sr = load_audiofile2(meta_data['filename'][i],cutoff_time)\n",
    "    training_data.append([get_melspectrogram(train_audio,n_fft=2048,hop_length=512,n_mels=128,fmin=40,fmax=15000,power=2.0,top_db=100),meta_data['common_name'][i]])\n",
    "    validation_data.append([get_melspectrogram(validation_audio,n_fft=2048,hop_length=512,n_mels=128,fmin=40,fmax=15000,power=2.0,top_db=100),meta_data['common_name'][i]])\n",
    "\n",
    "#Random selection of 100 data points, each point is taken in groups of 5\n",
    "selected_spots = random.sample(range(len(meta_data) - 4), 1)  \n",
    "\n",
    "for i in selected_spots:\n",
    "    for j in range(i, i + 4):\n",
    "        train_audio,validation_audio, sr = load_audiofile2(meta_data['filename'][i],cutoff_time)\n",
    "        training_data.append([get_melspectrogram(train_audio,n_fft=2048,hop_length=512,n_mels=128,fmin=40,fmax=15000,power=2.0,top_db=100),meta_data['common_name'][i]])\n",
    "        validation_data.append([get_melspectrogram(validation_audio,n_fft=2048,hop_length=512,n_mels=128,fmin=40,fmax=15000,power=2.0,top_db=100),meta_data['common_name'][i]])\n",
    "\n",
    "\n",
    "training_data = np.asarray(training_data)\n",
    "validation_data = np.asarray(validation_data)\n",
    "label_mapping = {label: index for index, label in enumerate(set(meta_data['common_name'].unique()))}\n",
    "training_data[:,1] = [label_mapping.get(label, -1)+1 for label in training_data[:,1]]\n",
    "validation_data[:,1] = [label_mapping.get(label, -1)+1 for label in validation_data[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "validation_data = []\n",
    "cutoff_time = 30\n",
    "\n",
    "for common_name in meta_data['common_name'].unique():\n",
    "    i = meta_data.loc[meta_data['common_name'] == common_name].index[0] \n",
    "    train_audio,validation_audio, sr = load_audiofile2(meta_data['filename'][i],cutoff_time)\n",
    "    training_data.append([get_melspectrogram(train_audio,n_fft=2048,hop_length=512,n_mels=128,fmin=40,fmax=15000,power=2.0,top_db=100),meta_data['common_name'][i]])\n",
    "    validation_data.append([get_melspectrogram(validation_audio,n_fft=2048,hop_length=512,n_mels=128,fmin=40,fmax=15000,power=2.0,top_db=100),meta_data['common_name'][i]])\n",
    "\n",
    "training_data = np.asarray(training_data)\n",
    "validation_data = np.asarray(validation_data)\n",
    "label_mapping = {label: index for index, label in enumerate(set(meta_data['common_name'].unique()))}\n",
    "training_data[:,1] = [label_mapping.get(label, -1)+1 for label in training_data[:,1]]\n",
    "validation_data[:,1] = [label_mapping.get(label, -1)+1 for label in validation_data[:,1]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectrogram extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate spectrograms\n",
    "spectrograms = []\n",
    "for i in range(50):\n",
    "    spectrograms.append([get_melspectrogram(meta_data['filename'][i]),meta_data['common_name'][i]])\n",
    "spectrograms = np.asarray(spectrograms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load one of each birds data\n",
    "spectrograms = []\n",
    "\n",
    "for common_name in meta_data['common_name'].unique():\n",
    "    index = meta_data.loc[meta_data['common_name'] == common_name].index[0] \n",
    "    spectrogram = get_melspectrogram(meta_data['filename'][index])  \n",
    "    spectrograms.append([spectrogram, common_name])\n",
    "\n",
    "spectrograms = np.asarray(spectrograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#Random selection of 100 data points, each point is taken in groups of 5\n",
    "spectrograms = []\n",
    "selected_spots = random.sample(range(len(meta_data) - 4), 100)  \n",
    "\n",
    "for i in selected_spots:\n",
    "    for j in range(i, i + 4):\n",
    "        spectrograms.append([get_melspectrogram(meta_data['filename'][j]), meta_data['common_name'][j]])\n",
    "\n",
    "spectrograms = np.asarray(spectrograms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate audio data\n",
    "training_data = []\n",
    "validation_data = []\n",
    "for i in range(50):\n",
    "    train_audio,validation_audio, sr = load_audiofile(meta_data['filename'][i],35)\n",
    "    training_data.append([get_melspectrogram(train_audio),meta_data['common_name'][i]])\n",
    "    validation_data.append([get_melspectrogram(validation_audio),meta_data['common_name'][i]])\n",
    "\n",
    "training_data = np.asarray(training_data)\n",
    "validation_data = np.asarray(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into validation and test. (Need to look at this again, possible mistake in the validation data creation)\n",
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * len(spectrograms))\n",
    "\n",
    "train_labels = spectrograms[:,1]\n",
    "\n",
    "label_mapping = {label: index for index, label in enumerate(set(train_labels))}\n",
    "\n",
    "spectrograms[:,1] = [label_mapping.get(label, -1)+1 for label in train_labels]\n",
    "\n",
    "validation_set = []\n",
    "training_set = []\n",
    "\n",
    "#Split data \n",
    "for i, (spectrogram, label) in enumerate(spectrograms):\n",
    "    shape = np.shape(spectrogram)\n",
    "    if shape[1] >= 100:\n",
    "        validation_data = spectrogram[:, :50]\n",
    "        validation_set.append([validation_data, label])\n",
    "        \n",
    "        remaining_data = spectrogram[:, 50:]\n",
    "        num_chunks = remaining_data.shape[1] // 50\n",
    "        if num_chunks > 0:\n",
    "            chunks = np.split(remaining_data[:, :num_chunks*50], num_chunks, axis=1)\n",
    "            for chunk in chunks:\n",
    "                training_set.append([chunk, label])\n",
    "    else: print(i,label)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 264\n",
      "2 / 264\n",
      "3 / 264\n",
      "4 / 264\n",
      "5 / 264\n",
      "6 / 264\n",
      "7 / 264\n",
      "8 / 264\n",
      "9 / 264\n",
      "10 / 264\n",
      "11 / 264\n",
      "12 / 264\n",
      "13 / 264\n",
      "14 / 264\n",
      "15 / 264\n",
      "16 / 264\n",
      "17 / 264\n",
      "18 / 264\n",
      "19 / 264\n",
      "20 / 264\n",
      "21 / 264\n",
      "22 / 264\n",
      "23 / 264\n",
      "24 / 264\n",
      "25 / 264\n",
      "26 / 264\n",
      "27 / 264\n",
      "28 / 264\n",
      "29 / 264\n",
      "30 / 264\n",
      "31 / 264\n",
      "32 / 264\n",
      "33 / 264\n",
      "34 / 264\n",
      "35 / 264\n",
      "36 / 264\n",
      "37 / 264\n",
      "38 / 264\n",
      "39 / 264\n",
      "40 / 264\n",
      "41 / 264\n",
      "42 / 264\n",
      "43 / 264\n",
      "44 / 264\n",
      "45 / 264\n",
      "46 / 264\n",
      "47 / 264\n",
      "48 / 264\n",
      "49 / 264\n",
      "50 / 264\n",
      "51 / 264\n",
      "52 / 264\n",
      "53 / 264\n",
      "54 / 264\n",
      "55 / 264\n",
      "56 / 264\n",
      "57 / 264\n",
      "58 / 264\n",
      "59 / 264\n",
      "60 / 264\n",
      "61 / 264\n",
      "62 / 264\n",
      "63 / 264\n",
      "64 / 264\n",
      "65 / 264\n",
      "66 / 264\n",
      "67 / 264\n",
      "68 / 264\n",
      "69 / 264\n",
      "70 / 264\n",
      "71 / 264\n",
      "72 / 264\n",
      "73 / 264\n",
      "74 / 264\n",
      "75 / 264\n",
      "76 / 264\n",
      "77 / 264\n",
      "78 / 264\n",
      "79 / 264\n",
      "80 / 264\n",
      "81 / 264\n",
      "82 / 264\n",
      "83 / 264\n",
      "84 / 264\n",
      "85 / 264\n",
      "86 / 264\n",
      "87 / 264\n",
      "88 / 264\n",
      "89 / 264\n",
      "90 / 264\n",
      "91 / 264\n",
      "92 / 264\n",
      "93 / 264\n",
      "94 / 264\n",
      "95 / 264\n",
      "96 / 264\n",
      "97 / 264\n",
      "98 / 264\n",
      "99 / 264\n",
      "100 / 264\n",
      "101 / 264\n",
      "102 / 264\n",
      "103 / 264\n",
      "104 / 264\n",
      "105 / 264\n",
      "106 / 264\n",
      "107 / 264\n",
      "108 / 264\n",
      "109 / 264\n",
      "110 / 264\n",
      "111 / 264\n",
      "112 / 264\n",
      "113 / 264\n",
      "114 / 264\n",
      "115 / 264\n",
      "116 / 264\n",
      "117 / 264\n",
      "118 / 264\n",
      "119 / 264\n",
      "120 / 264\n",
      "121 / 264\n",
      "122 / 264\n",
      "123 / 264\n",
      "124 / 264\n",
      "125 / 264\n",
      "126 / 264\n",
      "127 / 264\n",
      "128 / 264\n",
      "129 / 264\n",
      "130 / 264\n",
      "131 / 264\n",
      "132 / 264\n",
      "133 / 264\n",
      "134 / 264\n",
      "135 / 264\n",
      "136 / 264\n",
      "137 / 264\n",
      "138 / 264\n",
      "139 / 264\n",
      "140 / 264\n",
      "141 / 264\n",
      "142 / 264\n",
      "143 / 264\n",
      "144 / 264\n",
      "145 / 264\n",
      "146 / 264\n",
      "147 / 264\n",
      "148 / 264\n",
      "149 / 264\n",
      "150 / 264\n",
      "151 / 264\n",
      "152 / 264\n",
      "153 / 264\n",
      "154 / 264\n",
      "155 / 264\n",
      "156 / 264\n",
      "157 / 264\n",
      "158 / 264\n",
      "159 / 264\n",
      "160 / 264\n",
      "161 / 264\n",
      "162 / 264\n",
      "163 / 264\n",
      "164 / 264\n",
      "165 / 264\n",
      "166 / 264\n",
      "167 / 264\n",
      "168 / 264\n",
      "169 / 264\n",
      "170 / 264\n",
      "171 / 264\n",
      "172 / 264\n",
      "173 / 264\n",
      "174 / 264\n",
      "175 / 264\n",
      "176 / 264\n",
      "177 / 264\n",
      "178 / 264\n",
      "179 / 264\n",
      "180 / 264\n",
      "181 / 264\n",
      "182 / 264\n",
      "183 / 264\n",
      "184 / 264\n",
      "185 / 264\n",
      "186 / 264\n",
      "187 / 264\n",
      "188 / 264\n",
      "189 / 264\n",
      "190 / 264\n",
      "191 / 264\n",
      "192 / 264\n",
      "193 / 264\n",
      "194 / 264\n",
      "195 / 264\n",
      "196 / 264\n",
      "197 / 264\n",
      "198 / 264\n",
      "199 / 264\n",
      "200 / 264\n",
      "201 / 264\n",
      "202 / 264\n",
      "203 / 264\n",
      "204 / 264\n",
      "205 / 264\n",
      "206 / 264\n",
      "207 / 264\n",
      "208 / 264\n",
      "209 / 264\n",
      "210 / 264\n",
      "211 / 264\n",
      "212 / 264\n",
      "213 / 264\n",
      "214 / 264\n",
      "215 / 264\n",
      "216 / 264\n",
      "217 / 264\n",
      "218 / 264\n",
      "219 / 264\n",
      "220 / 264\n",
      "221 / 264\n",
      "222 / 264\n",
      "223 / 264\n",
      "224 / 264\n",
      "225 / 264\n",
      "226 / 264\n",
      "227 / 264\n",
      "228 / 264\n",
      "229 / 264\n",
      "230 / 264\n",
      "231 / 264\n",
      "232 / 264\n",
      "233 / 264\n",
      "234 / 264\n",
      "235 / 264\n",
      "236 / 264\n",
      "237 / 264\n",
      "238 / 264\n",
      "239 / 264\n",
      "240 / 264\n",
      "241 / 264\n",
      "242 / 264\n",
      "243 / 264\n",
      "244 / 264\n",
      "245 / 264\n",
      "246 / 264\n",
      "247 / 264\n",
      "248 / 264\n",
      "249 / 264\n",
      "250 / 264\n",
      "251 / 264\n",
      "252 / 264\n",
      "253 / 264\n",
      "254 / 264\n",
      "255 / 264\n",
      "256 / 264\n",
      "257 / 264\n",
      "258 / 264\n",
      "259 / 264\n",
      "260 / 264\n",
      "261 / 264\n",
      "262 / 264\n",
      "263 / 264\n",
      "264 / 264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhakk\\AppData\\Local\\Temp\\ipykernel_16548\\4176385709.py:68: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  training_data = np.asarray(training_data)\n",
      "C:\\Users\\zhakk\\AppData\\Local\\Temp\\ipykernel_16548\\4176385709.py:68: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training_data = np.asarray(training_data)\n",
      "C:\\Users\\zhakk\\AppData\\Local\\Temp\\ipykernel_16548\\4176385709.py:69: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  validation_data = np.asarray(validation_data)\n",
      "C:\\Users\\zhakk\\AppData\\Local\\Temp\\ipykernel_16548\\4176385709.py:69: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  validation_data = np.asarray(validation_data)\n"
     ]
    }
   ],
   "source": [
    "# Define the duration of each segment in seconds\n",
    "segment_duration = 15\n",
    "max_files_per_bird = 5\n",
    "\n",
    "# Generate audio data\n",
    "training_data = []\n",
    "validation_data = []\n",
    "birds_with_single_clip = []\n",
    "\n",
    "# Iterate through each unique bird\n",
    "for i, bird in enumerate(meta_data['common_name'].unique()):\n",
    "    print(i+1, \"/\", len(meta_data['common_name'].unique()))\n",
    "    # Get all audio clips for the bird\n",
    "    bird_clips = meta_data[meta_data['common_name'] == bird]['filename'].tolist()\n",
    "    # If the bird has only one clip, split it into two halves\n",
    "    if len(bird_clips) == 1:\n",
    "        birds_with_single_clip.append(bird)\n",
    "    else:\n",
    "        # Randomly select one clip for validation and the rest for training\n",
    "        random.shuffle(bird_clips)\n",
    "        training_clip = bird_clips[1:]\n",
    "        validation_clip = bird_clips[0]\n",
    "\n",
    "    # Load and process the training clip\n",
    "    num_files_sampled = 0\n",
    "    for clip in training_clip:\n",
    "        if num_files_sampled >= max_files_per_bird:\n",
    "            break\n",
    "        train_audio, sr = load_audiofile(clip)\n",
    "        num_segments = math.floor(len(train_audio) / (segment_duration * sr))\n",
    "        for segment in range(num_segments):\n",
    "            start_time = segment * segment_duration\n",
    "            end_time = start_time + segment_duration\n",
    "            segment_audio = train_audio[start_time * sr:end_time * sr]\n",
    "            training_data.append([get_melspectrogram(segment_audio), bird])\n",
    "        num_files_sampled += 1\n",
    "\n",
    "    # Load and process the validation clip\n",
    "    validation_audio, sr = load_audiofile(validation_clip)\n",
    "    num_segments = math.floor(len(validation_audio) / (segment_duration * sr))\n",
    "    for segment in range(num_segments):\n",
    "        start_time = segment * segment_duration\n",
    "        end_time = start_time + segment_duration\n",
    "        segment_audio = validation_audio[start_time * sr:end_time * sr]\n",
    "        validation_data.append([get_melspectrogram(segment_audio), bird])\n",
    "\n",
    "# Split the single clips into training and validation\n",
    "random.shuffle(birds_with_single_clip)\n",
    "split_index = len(birds_with_single_clip) // 2\n",
    "training_single_clips = birds_with_single_clip[split_index:]\n",
    "validation_single_clips = birds_with_single_clip[:split_index]\n",
    "\n",
    "# Append the single clips to the training and validation data\n",
    "for bird in birds_with_single_clip:\n",
    "    if len(training_data) >= max_files_per_bird:\n",
    "        break\n",
    "    clip = meta_data[meta_data['common_name'] == bird]['filename'].tolist()[0]\n",
    "    audio, sr = load_audiofile(clip)\n",
    "    num_segments = math.floor(len(audio) / (segment_duration * sr))\n",
    "    for segment in range(num_segments):\n",
    "        start_time = segment * segment_duration\n",
    "        end_time = start_time + segment_duration\n",
    "        segment_audio = audio[start_time * sr:end_time * sr]\n",
    "        validation_data.append([get_melspectrogram(segment_audio), bird])\n",
    "        training_data.append([get_melspectrogram(segment_audio), bird])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "training_data = np.asarray(training_data)\n",
    "validation_data = np.asarray(validation_data)\n",
    "\n",
    "# Map labels to indices\n",
    "label_mapping = {label: index for index, label in enumerate(set(meta_data['common_name'].unique()))}\n",
    "training_data[:, 1] = [label_mapping.get(label, -1) + 1 for label in training_data[:, 1]]\n",
    "validation_data[:, 1] = [label_mapping.get(label, -1) + 1 for label in validation_data[:, 1]]\n",
    "\n",
    "# Clear temporary data and variables\n",
    "birds_with_single_clip = None\n",
    "training_single_clips = None\n",
    "validation_single_clips = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2178, 419)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data),len(validation_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From audiofiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(239616, num_classes)  # Initialize with size 0\n",
    "        self.fc2 = nn.Linear(num_classes, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        self.fc1 = nn.Linear(1280, num_classes)  # Update the input size\n",
    "        self.fc2 = nn.Linear(num_classes, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "epoch :  0\n"
     ]
    }
   ],
   "source": [
    "# Load data into batches of 32\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(training_data.tolist(), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(validation_data.tolist(), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the CNN and use +1 for classes due to \"no class\" being labeled as -1\n",
    "num_classes = len(meta_data['common_name'].unique()) + 1\n",
    "cnn = CNN(num_classes)\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "\n",
    "# Training loop and attempt to use CUDA\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cnn.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch : \", epoch)\n",
    "    cnn.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    i = 0\n",
    "    for images, labels in train_loader:\n",
    "        # Convert 1D images to 3D\n",
    "        images_rgb = torch.stack([images] * 3, dim=1)\n",
    "\n",
    "        # Load data onto device, either GPU or CPU\n",
    "        images_rgb = images_rgb.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = cnn.model(images_rgb)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        i += 1\n",
    "\n",
    "    # Validation loop\n",
    "    cnn.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "\n",
    "    with torch.no_grad():  # Disables some calculations, used to reduce memory.\n",
    "        for images, labels in val_loader:\n",
    "            # Convert 1D images to 3D\n",
    "            images_rgb = torch.stack([images] * 3, dim=1)\n",
    "\n",
    "            # Load data onto device, either GPU or CPU\n",
    "            images_rgb = images_rgb.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = cnn.model(images_rgb)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc = train_correct / len(train_loader.dataset)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = val_correct / len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Load data into batches of 32\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(training_data.tolist(), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(validation_data.tolist(), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the CNN and use +1 for classes due to \"no class\" being labeled as -1\n",
    "num_classes = len(meta_data['common_name'].unique())+1\n",
    "cnn = CNN(num_classes)\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "\n",
    "# Training loop and attempt to use cuda\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cnn.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch : \", epoch)\n",
    "    cnn.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    i = 0 \n",
    "    for images, labels in train_loader:\n",
    "        #load data onto device, either gpu or cpu\n",
    "        images = images.unsqueeze(1).to(device) \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        i+=1\n",
    "        \n",
    "    # Validation loop\n",
    "    cnn.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    \n",
    "    with torch.no_grad(): #Disables some calculations, used to reduce memory.\n",
    "        for images, labels in val_loader:\n",
    "        #load data onto device, either gpu or cpu\n",
    "            images = images.unsqueeze(1).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = cnn(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc = train_correct / len(train_loader.dataset)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = val_correct / len(val_loader.dataset)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(12288, num_classes)\n",
    "        self.fc2 = nn.Linear(num_classes, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zhakk\\Desktop\\Uni\\Kandidat\\AML-BirdCLEFproject\\AML-BirdCLEF\\birdCLEFNeuralNet.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zhakk/Desktop/Uni/Kandidat/AML-BirdCLEFproject/AML-BirdCLEF/birdCLEFNeuralNet.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#Load data into batches of 32\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zhakk/Desktop/Uni/Kandidat/AML-BirdCLEFproject/AML-BirdCLEF/birdCLEFNeuralNet.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zhakk/Desktop/Uni/Kandidat/AML-BirdCLEFproject/AML-BirdCLEF/birdCLEFNeuralNet.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(training_set, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zhakk/Desktop/Uni/Kandidat/AML-BirdCLEFproject/AML-BirdCLEF/birdCLEFNeuralNet.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m val_loader \u001b[39m=\u001b[39m DataLoader(validation_set, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zhakk/Desktop/Uni/Kandidat/AML-BirdCLEFproject/AML-BirdCLEF/birdCLEFNeuralNet.ipynb#X41sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Initialize the CNN and use +1 for classes due to \"no class\" being labeled as -1\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'training_set' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#Load data into batches of 32\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the CNN and use +1 for classes due to \"no class\" being labeled as -1\n",
    "num_classes = len(meta_data['common_name'].unique())+1\n",
    "cnn = CNN(num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop and attempt to use cuda\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cnn.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch : \", epoch)\n",
    "    cnn.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    i = 0 \n",
    "    for images, labels in train_loader:\n",
    "        #load data onto device, either gpu or cpu\n",
    "        images = images.unsqueeze(1).to(device) \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        i+=1\n",
    "        \n",
    "    # Validation loop\n",
    "    cnn.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    \n",
    "    with torch.no_grad(): #Disables some calculations, used to reduce memory.\n",
    "        for images, labels in val_loader:\n",
    "        #load data onto device, either gpu or cpu\n",
    "            images = images.unsqueeze(1).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = cnn(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc = train_correct / len(train_loader.dataset)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = val_correct / len(val_loader.dataset)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "APP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
