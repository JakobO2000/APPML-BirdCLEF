{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import Resize\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "from scipy.signal import stft\n",
    "import glob\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the directory with the data and returns pandas with metadata\n",
    "def load_metadata(directory, trim=False):\n",
    "    if trim:\n",
    "        df = pd.read_csv(directory+'/train_metadata_trim.csv')\n",
    "    else:\n",
    "        df = pd.read_csv(directory+'/train_metadata.csv')\n",
    "    df['filename'] = directory+\"/train_audio/\"+df['filename']\n",
    "    chosen_coloumns = ['latitude', 'longitude', 'common_name', 'rating', 'filename', 'primary_label']\n",
    "    return df[chosen_coloumns]\n",
    "\n",
    "\n",
    "# Takes filepath from metadata dataframe and returns audio file\n",
    "def load_audiofile(filepath):\n",
    "    audio, sr = sf.read(filepath)\n",
    "    return audio.astype(np.float32), sr\n",
    "\n",
    "\n",
    "# Converts ogg audio to waveform and spectrogram. Exact values for melspectrogram function might need to be changed values currently chosen from https://www.kaggle.com/code/awsaf49/birdclef23-pretraining-is-all-you-need-train\n",
    "# audio -- Can be filepath from metadata dataframe or numpy array with ogg data\n",
    "def get_melspectrogram(audio, sr=32000, n_mels=128, n_fft=2028, hop_length=512, fmax=16000, fmin=20,power=2.0,top_db=100):\n",
    "    if type(audio) is str:\n",
    "        audio, sr = load_audiofile(audio)\n",
    "    waveform = torch.from_numpy(audio)\n",
    "    transform = torchaudio.transforms.MelSpectrogram( \n",
    "                                    sample_rate=sr, \n",
    "                                    n_mels=n_mels,\n",
    "                                    n_fft=n_fft,\n",
    "                                    hop_length=hop_length, #base value from function in notebook it is calculated as duration_of_audio*sr//(384-1)\n",
    "                                    f_max=fmax,\n",
    "                                    f_min=fmin,\n",
    "                                    power=2.0\n",
    "                                    )\n",
    "    melspectrogram = transform(waveform)\n",
    "\n",
    "    melspectrogram = torchaudio.transforms.AmplitudeToDB()(melspectrogram)\n",
    "    melspectrogram = torch.nn.functional.normalize(melspectrogram, p=2, dim=0)\n",
    "    \n",
    "    melspectrogram = (melspectrogram * 255)\n",
    "\n",
    "    return melspectrogram\n",
    "\n",
    "#Calculates Short Time Fourier Transformation of an audio file\n",
    "# audio -- Can be filepath from metadata dataframe or numpy array with ogg data\n",
    "def get_STFT(audio, sr=32000, n_fft=2028, nperseg=512):\n",
    "    if type(audio) is str:\n",
    "        audio, sr = load_audiofile(audio)\n",
    "    stft_audio = stft(audio, nfft=n_fft, nperseg=nperseg)\n",
    "    return stft_audio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "path = \"D:\\KU\\Masters\\AppML\\APPML-BirdCLEF\\data\"\n",
    "meta_data = load_metadata(path, trim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audiofile2(filepath, cutoff_time):\n",
    "    #Function that takes a file and cutoff time to create training and validation sets for training\n",
    "    #If the audioclip is lower than the cutoff_time then the clip is looped untill desired duration reached\n",
    "    audio, sr = sf.read(filepath)\n",
    "    duration = len(audio) / sr\n",
    "\n",
    "\n",
    "    if duration >= cutoff_time:\n",
    "        training_audio = audio[:int(sr * 15)]\n",
    "        validation_audio = audio[int(sr * 15):int(sr * 30)]\n",
    "    else:\n",
    "        \n",
    "        loop_count = int(np.ceil(cutoff_time / duration))\n",
    "        audio = np.tile(audio, loop_count)\n",
    "\n",
    "        training_audio = audio[:int(sr * 15)]\n",
    "        validation_audio = audio[int(sr * 15):int(sr * 30)]\n",
    "\n",
    "    return training_audio.astype(np.float32), validation_audio.astype(np.float32), sr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrogram extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Birk Dissing\\AppData\\Local\\Temp\\ipykernel_27488\\814870280.py:15: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  spectrograms = np.asarray(spectrograms)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (664, 2) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(i, i \u001b[39m+\u001b[39m \u001b[39m4\u001b[39m):\n\u001b[0;32m     13\u001b[0m         spectrograms\u001b[39m.\u001b[39mappend([get_melspectrogram(meta_data[\u001b[39m'\u001b[39m\u001b[39mfilename\u001b[39m\u001b[39m'\u001b[39m][j]), meta_data[\u001b[39m'\u001b[39m\u001b[39mprimary_label\u001b[39m\u001b[39m'\u001b[39m][j]])\n\u001b[1;32m---> 15\u001b[0m spectrograms \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(spectrograms)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (664, 2) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import random\n",
    "#load one of each birds data\n",
    "spectrograms = []\n",
    "selected_spots = random.sample(range(len(meta_data) - 4), 100)  \n",
    "\n",
    "for common_name in meta_data['primary_label'].unique():\n",
    "    index = meta_data.loc[meta_data['primary_label'] == common_name].index[0] \n",
    "    spectrogram = get_melspectrogram(meta_data['filename'][index])  \n",
    "    spectrograms.append([spectrogram, common_name])\n",
    "\n",
    "for i in selected_spots:\n",
    "    for j in range(i, i + 4):\n",
    "        spectrograms.append([get_melspectrogram(meta_data['filename'][j]), meta_data['primary_label'][j]])\n",
    "\n",
    "spectrograms = np.asarray(spectrograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605 188\n",
      "645 95\n"
     ]
    }
   ],
   "source": [
    "#Split data into validation and test. (Need to look at this again, possible mistake in the validation data creation)\n",
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * len(spectrograms))\n",
    "\n",
    "train_labels = spectrograms[:,1]\n",
    "\n",
    "label_mapping = {label: index for index, label in enumerate(set(train_labels))}\n",
    "\n",
    "spectrograms[:,1] = [label_mapping.get(label, -1)+1 for label in train_labels]\n",
    "\n",
    "validation_set = []\n",
    "training_set = []\n",
    "\n",
    "#Split data \n",
    "for i, (spectrogram, label) in enumerate(spectrograms):\n",
    "    shape = np.shape(spectrogram)\n",
    "    if shape[1] >= 100:\n",
    "        validation_data = spectrogram[:, :50]\n",
    "        validation_set.append([validation_data, label])\n",
    "        \n",
    "        remaining_data = spectrogram[:, 50:]\n",
    "        num_chunks = remaining_data.shape[1] // 50\n",
    "        if num_chunks > 0:\n",
    "            chunks = np.split(remaining_data[:, :num_chunks*50], num_chunks, axis=1)\n",
    "            for chunk in chunks:\n",
    "                training_set.append([chunk, label])\n",
    "    else: print(i,label)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(12288, num_classes)\n",
    "        self.fc2 = nn.Linear(num_classes, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        size = x.shape[1] * x.shape[2] * x.shape[3]\n",
    "        x = x.view(x.size(0), -1)\n",
    "        self.fc1 = nn.Linear(size, num_classes)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(12288, num_classes)\n",
    "        self.fc2 = nn.Linear(num_classes, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0\n",
      "Epoch 1/10:\n",
      "Train Loss: 1.8677 | Train Acc: 0.6185\n",
      "Val Loss: 2.2717 | Val Acc: 0.5076\n",
      "epoch :  1\n",
      "Epoch 2/10:\n",
      "Train Loss: 0.3637 | Train Acc: 0.9002\n",
      "Val Loss: 1.8863 | Val Acc: 0.6042\n",
      "epoch :  2\n",
      "Epoch 3/10:\n",
      "Train Loss: 0.1959 | Train Acc: 0.9447\n",
      "Val Loss: 2.1042 | Val Acc: 0.5967\n",
      "epoch :  3\n",
      "Epoch 4/10:\n",
      "Train Loss: 0.1379 | Train Acc: 0.9615\n",
      "Val Loss: 2.3378 | Val Acc: 0.5770\n",
      "epoch :  4\n",
      "Epoch 5/10:\n",
      "Train Loss: 0.1186 | Train Acc: 0.9662\n",
      "Val Loss: 2.0243 | Val Acc: 0.6148\n",
      "epoch :  5\n",
      "Epoch 6/10:\n",
      "Train Loss: 0.1042 | Train Acc: 0.9700\n",
      "Val Loss: 1.7906 | Val Acc: 0.6918\n",
      "epoch :  6\n",
      "Epoch 7/10:\n",
      "Train Loss: 0.0947 | Train Acc: 0.9742\n",
      "Val Loss: 2.2409 | Val Acc: 0.6329\n",
      "epoch :  7\n",
      "Epoch 8/10:\n",
      "Train Loss: 0.0756 | Train Acc: 0.9787\n",
      "Val Loss: 2.1307 | Val Acc: 0.6495\n",
      "epoch :  8\n",
      "Epoch 9/10:\n",
      "Train Loss: 0.0713 | Train Acc: 0.9797\n",
      "Val Loss: 2.4943 | Val Acc: 0.6148\n",
      "epoch :  9\n",
      "Epoch 10/10:\n",
      "Train Loss: 0.0603 | Train Acc: 0.9821\n",
      "Val Loss: 2.2951 | Val Acc: 0.6631\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Load data into batches of 32\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the CNN and use +1 for classes due to \"no class\" being labeled as -1\n",
    "num_classes = len(meta_data['primary_label'].unique())+1\n",
    "cnn = CNN(num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop and attempt to use cuda\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cpu\")\n",
    "cnn.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch : \", epoch)\n",
    "    cnn.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    i = 0 \n",
    "    for images, labels in train_loader:\n",
    "        #load data onto device, either gpu or cpu\n",
    "        images = images.unsqueeze(1).to(device) \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        i+=1\n",
    "        \n",
    "    # Validation loop\n",
    "    cnn.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    \n",
    "    with torch.no_grad(): #Disables some calculations, used to reduce memory.\n",
    "        for images, labels in val_loader:\n",
    "        #load data onto device, either gpu or cpu\n",
    "            images = images.unsqueeze(1).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = cnn(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc = train_correct / len(train_loader.dataset)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = val_correct / len(val_loader.dataset)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio(audio, sr, segment_duration=30):\n",
    "    segment_length = segment_duration * sr  # Length of each segment in samples\n",
    "    total_samples = len(audio)\n",
    "    num_segments = total_samples // segment_length\n",
    "\n",
    "    segments = []\n",
    "    for i in range(num_segments):\n",
    "        segment_start = i * segment_length\n",
    "        segment_end = segment_start + segment_length\n",
    "        segment = audio[segment_start:segment_end]\n",
    "        segments.append([segment,i*segment_duration])\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soundscape_29201.ogg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "arr = os.listdir(test_path)\n",
    "\n",
    "for file in arr:\n",
    "    print(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data saved to submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhakk\\AppData\\Local\\Temp\\ipykernel_16996\\652787451.py:13: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  spectrograms = np.asarray(spectrograms)\n",
      "C:\\Users\\zhakk\\AppData\\Local\\Temp\\ipykernel_16996\\652787451.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  spectrograms = np.asarray(spectrograms)\n",
      "C:\\Users\\zhakk\\AppData\\Local\\Temp\\ipykernel_16996\\652787451.py:25: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  test_set = np.array(test_set)\n",
      "C:\\Users\\zhakk\\AppData\\Local\\Temp\\ipykernel_16996\\652787451.py:25: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  test_set = np.array(test_set)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "test_path = r\"C:\\Users\\zhakk\\Desktop\\Uni\\Kandidat\\AML-BirdCLEFproject\\data\\birdCLEF2023\\test_soundscapes\"\n",
    "\n",
    "for file in arr:\n",
    "    audio,sr = load_audiofile(test_path+\"/\"+file)\n",
    "    audio_segment = split_audio(audio,sr)\n",
    "    spectrograms = []\n",
    "    for i in audio_segment:\n",
    "        spectrogram = get_melspectrogram(i[0])  \n",
    "        spectrograms.append([spectrogram, i[1]])\n",
    "\n",
    "    spectrograms = np.asarray(spectrograms)\n",
    "    test_set = []\n",
    "\n",
    "    for i, (spectrogram, time) in enumerate(spectrograms):\n",
    "        shape = np.shape(spectrogram)\n",
    "        if shape[1] >= 100:\n",
    "            test_data = spectrogram[:, :50]  # Use the first 50 time steps as test data\n",
    "            test_set.append([test_data, time])\n",
    "        else:\n",
    "            print(i, label)\n",
    "\n",
    "    # Convert the test data to a numpy array for easier manipulation\n",
    "    test_set = np.array(test_set)\n",
    "\n",
    "\n",
    "    # Separate the test features and labels\n",
    "    test_features = test_set[:, 0]\n",
    "    test_times = test_set[:, 1]\n",
    "    \n",
    "    test_loader = DataLoader(test_features, batch_size=batch_size, shuffle=False)\n",
    "    cnn.eval()\n",
    "\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for images in test_loader:\n",
    "            images = images.unsqueeze(1).to(device)\n",
    "            outputs = cnn(images)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            predictions.extend(probabilities.tolist())\n",
    "\n",
    "    test_data = []\n",
    "\n",
    "    for start_time, probabilities in zip(test_times, predictions):\n",
    "        row_id = file + '_' + str(start_time)\n",
    "        test_data_row = [row_id] + probabilities\n",
    "\n",
    "        test_data.append(test_data_row)\n",
    "\n",
    "    # Write test data to a CSV file\n",
    "    output_file = 'submission.csv'\n",
    "    header = ['row_id'] + list(label_mapping.keys())\n",
    "\n",
    "    with open(output_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(test_data)\n",
    "\n",
    "    print('Test data saved to', output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "APP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
