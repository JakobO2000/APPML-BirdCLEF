{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from birdCLEFloaddata import load_audiofile,load_metadata,get_melspectrogram\n",
    "from birdCLEFFunctions import Dynamic_CNN, Dynamic_CNN2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import Resize\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import soundfile as sf\n",
    "import random\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "path = r\"C:\\Users\\zhakk\\Desktop\\Uni\\Kandidat\\AML-BirdCLEFproject\\data\\birdCLEF2023\"\n",
    "\n",
    "meta_data = load_metadata(path,trim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audiofile2(filepath, cutoff_time):\n",
    "    #Function that takes a file and cutoff time to create training and validation sets for training\n",
    "    #If the audioclip is lower than the cutoff_time then the clip is looped untill desired duration reached\n",
    "    audio, sr = sf.read(filepath)\n",
    "    duration = len(audio) / sr\n",
    "\n",
    "\n",
    "    if duration >= cutoff_time:\n",
    "        training_audio = audio[:int(sr * 15)]\n",
    "        validation_audio = audio[int(sr * 15):int(sr * 30)]\n",
    "    else:\n",
    "        \n",
    "        loop_count = int(np.ceil(cutoff_time / duration))\n",
    "        audio = np.tile(audio, loop_count)\n",
    "\n",
    "        training_audio = audio[:int(sr * 15)]\n",
    "        validation_audio = audio[int(sr * 15):int(sr * 30)]\n",
    "\n",
    "    return training_audio.astype(np.float32), validation_audio.astype(np.float32), sr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First/simple data extraction methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate audio data\n",
    "training_data = []\n",
    "validation_data = []\n",
    "for i in range(50):\n",
    "    train_audio,validation_audio, sr = load_audiofile2(meta_data['filename'][i],30)\n",
    "    training_data.append([get_melspectrogram(train_audio),meta_data['common_name'][i]])\n",
    "    validation_data.append([get_melspectrogram(validation_audio),meta_data['common_name'][i]])\n",
    "\n",
    "\n",
    "training_data = np.asarray(training_data)\n",
    "validation_data = np.asarray(validation_data)\n",
    "label_mapping = {label: index for index, label in enumerate(set(meta_data['common_name'].unique()))}\n",
    "training_data[:,1] = [label_mapping.get(label, -1)+1 for label in training_data[:,1]]\n",
    "validation_data[:,1] = [label_mapping.get(label, -1)+1 for label in validation_data[:,1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Generate audio data\n",
    "training_data = []\n",
    "validation_data = []\n",
    "birds_with_single_clip = []\n",
    "\n",
    "# Iterate through each unique bird\n",
    "for bird in meta_data['common_name'].unique():\n",
    "    print(bird)\n",
    "    # Get all audio clips for the bird\n",
    "    bird_clips = meta_data[meta_data['common_name'] == bird]['filename'].tolist()\n",
    "    # If the bird has only one clip, split it into two halves\n",
    "    if len(bird_clips) == 1:\n",
    "        birds_with_single_clip.append(bird)\n",
    "    else:\n",
    "        # Randomly select one clip for validation and the rest for training\n",
    "        random.shuffle(bird_clips)\n",
    "        training_clip = bird_clips[1:]\n",
    "        validation_clip = bird_clips[0]\n",
    "    \n",
    "    # Load and process the training clip\n",
    "    for clip in training_clip:\n",
    "        train_audio, sr = load_audiofile(clip)\n",
    "        training_data.append([get_melspectrogram(train_audio), bird])\n",
    "    \n",
    "    # Load and process the validation clip\n",
    "    validation_audio, sr = load_audiofile(validation_clip)\n",
    "    validation_data.append([get_melspectrogram(validation_audio), bird])\n",
    "\n",
    "# Split the single clips into training and validation\n",
    "random.shuffle(birds_with_single_clip)\n",
    "split_index = len(birds_with_single_clip) // 2\n",
    "training_single_clips = birds_with_single_clip[split_index:]\n",
    "validation_single_clips = birds_with_single_clip[:split_index]\n",
    "\n",
    "# Append the single clips to the training and validation data\n",
    "for bird in birds_with_single_clip:\n",
    "    clip = meta_data[meta_data['common_name'] == bird]['filename'].tolist()[0]\n",
    "    audio, sr = load_audiofile(clip)\n",
    "    split_index = len(audio) // 2\n",
    "    val_audio = audio[:split_index]\n",
    "    train_audio = audio[split_index:]\n",
    "    validation_data.append([get_melspectrogram(val_audio), bird])\n",
    "    training_data.append([get_melspectrogram(train_audio), bird])\n",
    "\n",
    "\n",
    "# Convert to numpy arrays\n",
    "training_data = np.asarray(training_data)\n",
    "validation_data = np.asarray(validation_data)\n",
    "\n",
    "# Map labels to indices\n",
    "label_mapping = {label: index for index, label in enumerate(set(meta_data['common_name'].unique()))}\n",
    "training_data[:, 1] = [label_mapping.get(label, -1) + 1 for label in training_data[:, 1]]\n",
    "validation_data[:, 1] = [label_mapping.get(label, -1) + 1 for label in validation_data[:, 1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_audio.ndim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#Generate audio data\n",
    "training_data = []\n",
    "validation_data = []\n",
    "cutoff_time = 30\n",
    "\n",
    "for common_name in meta_data['common_name'].unique():\n",
    "    i = meta_data.loc[meta_data['common_name'] == common_name].index[0] \n",
    "    train_audio,validation_audio, sr = load_audiofile2(meta_data['filename'][i],cutoff_time)\n",
    "    training_data.append([get_melspectrogram(train_audio,n_fft=2048,hop_length=512,n_mels=128,fmin=40,fmax=15000,power=2.0,top_db=100),meta_data['common_name'][i]])\n",
    "    validation_data.append([get_melspectrogram(validation_audio,n_fft=2048,hop_length=512,n_mels=128,fmin=40,fmax=15000,power=2.0,top_db=100),meta_data['common_name'][i]])\n",
    "\n",
    "#Random selection of 100 data points, each point is taken in groups of 5\n",
    "selected_spots = random.sample(range(len(meta_data) - 4), 1)  \n",
    "\n",
    "for i in selected_spots:\n",
    "    for j in range(i, i + 4):\n",
    "        train_audio,validation_audio, sr = load_audiofile2(meta_data['filename'][i],cutoff_time)\n",
    "        training_data.append([get_melspectrogram(train_audio,n_fft=2048,hop_length=512,n_mels=128,fmin=40,fmax=15000,power=2.0,top_db=100),meta_data['common_name'][i]])\n",
    "        validation_data.append([get_melspectrogram(validation_audio,n_fft=2048,hop_length=512,n_mels=128,fmin=40,fmax=15000,power=2.0,top_db=100),meta_data['common_name'][i]])\n",
    "\n",
    "\n",
    "training_data = np.asarray(training_data)\n",
    "validation_data = np.asarray(validation_data)\n",
    "label_mapping = {label: index for index, label in enumerate(set(meta_data['common_name'].unique()))}\n",
    "training_data[:,1] = [label_mapping.get(label, -1)+1 for label in training_data[:,1]]\n",
    "validation_data[:,1] = [label_mapping.get(label, -1)+1 for label in validation_data[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "validation_data = []\n",
    "cutoff_time = 30\n",
    "\n",
    "for common_name in meta_data['common_name'].unique():\n",
    "    i = meta_data.loc[meta_data['common_name'] == common_name].index[0] \n",
    "    train_audio,validation_audio, sr = load_audiofile2(meta_data['filename'][i],cutoff_time)\n",
    "    training_data.append([get_melspectrogram(train_audio,n_fft=2048,hop_length=512,n_mels=128,fmin=40,fmax=15000,power=2.0,top_db=100),meta_data['common_name'][i]])\n",
    "    validation_data.append([get_melspectrogram(validation_audio,n_fft=2048,hop_length=512,n_mels=128,fmin=40,fmax=15000,power=2.0,top_db=100),meta_data['common_name'][i]])\n",
    "\n",
    "training_data = np.asarray(training_data)\n",
    "validation_data = np.asarray(validation_data)\n",
    "label_mapping = {label: index for index, label in enumerate(set(meta_data['common_name'].unique()))}\n",
    "training_data[:,1] = [label_mapping.get(label, -1)+1 for label in training_data[:,1]]\n",
    "validation_data[:,1] = [label_mapping.get(label, -1)+1 for label in validation_data[:,1]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectrogram extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate spectrograms\n",
    "spectrograms = []\n",
    "for i in range(50):\n",
    "    spectrograms.append([get_melspectrogram(meta_data['filename'][i]),meta_data['common_name'][i]])\n",
    "spectrograms = np.asarray(spectrograms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load one of each birds data\n",
    "spectrograms = []\n",
    "\n",
    "for common_name in meta_data['common_name'].unique():\n",
    "    index = meta_data.loc[meta_data['common_name'] == common_name].index[0] \n",
    "    spectrogram = get_melspectrogram(meta_data['filename'][index])  \n",
    "    spectrograms.append([spectrogram, common_name])\n",
    "\n",
    "spectrograms = np.asarray(spectrograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#Random selection of 100 data points, each point is taken in groups of 5\n",
    "spectrograms = []\n",
    "selected_spots = random.sample(range(len(meta_data) - 4), 100)  \n",
    "\n",
    "for i in selected_spots:\n",
    "    for j in range(i, i + 4):\n",
    "        spectrograms.append([get_melspectrogram(meta_data['filename'][j]), meta_data['common_name'][j]])\n",
    "\n",
    "spectrograms = np.asarray(spectrograms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate audio data\n",
    "training_data = []\n",
    "validation_data = []\n",
    "for i in range(50):\n",
    "    train_audio,validation_audio, sr = load_audiofile(meta_data['filename'][i],35)\n",
    "    training_data.append([get_melspectrogram(train_audio),meta_data['common_name'][i]])\n",
    "    validation_data.append([get_melspectrogram(validation_audio),meta_data['common_name'][i]])\n",
    "\n",
    "training_data = np.asarray(training_data)\n",
    "validation_data = np.asarray(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into validation and test. (Need to look at this again, possible mistake in the validation data creation)\n",
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * len(spectrograms))\n",
    "\n",
    "train_labels = spectrograms[:,1]\n",
    "\n",
    "label_mapping = {label: index for index, label in enumerate(set(train_labels))}\n",
    "\n",
    "spectrograms[:,1] = [label_mapping.get(label, -1)+1 for label in train_labels]\n",
    "\n",
    "validation_set = []\n",
    "training_set = []\n",
    "\n",
    "#Split data \n",
    "for i, (spectrogram, label) in enumerate(spectrograms):\n",
    "    shape = np.shape(spectrogram)\n",
    "    if shape[1] >= 100:\n",
    "        validation_data = spectrogram[:, :50]\n",
    "        validation_set.append([validation_data, label])\n",
    "        \n",
    "        remaining_data = spectrogram[:, 50:]\n",
    "        num_chunks = remaining_data.shape[1] // 50\n",
    "        if num_chunks > 0:\n",
    "            chunks = np.split(remaining_data[:, :num_chunks*50], num_chunks, axis=1)\n",
    "            for chunk in chunks:\n",
    "                training_set.append([chunk, label])\n",
    "    else: print(i,label)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Generate audio data\n",
    "training_data = []\n",
    "validation_data = []\n",
    "birds_with_single_clip = []\n",
    "\n",
    "# Iterate through each unique bird\n",
    "for i, bird in enumerate(meta_data['common_name'].unique()):\n",
    "    print(i,\"/\",len(meta_data['common_name'].unique()))\n",
    "    # Get all audio clips for the bird\n",
    "    bird_clips = meta_data[meta_data['common_name'] == bird]['filename'].tolist()\n",
    "    # If the bird has only one clip, split it into two halves\n",
    "    if len(bird_clips) == 1:\n",
    "        birds_with_single_clip.append(bird)\n",
    "    else:\n",
    "        # Randomly select one clip for validation and the rest for training\n",
    "        random.shuffle(bird_clips)\n",
    "        training_clip = bird_clips[1:]\n",
    "        validation_clip = bird_clips[0]\n",
    "    \n",
    "    # Load and process the training clip\n",
    "    for clip in training_clip:\n",
    "        train_audio, sr = load_audiofile(clip)\n",
    "        training_data.append([get_melspectrogram(train_audio), bird])\n",
    "    \n",
    "    # Load and process the validation clip\n",
    "    validation_audio, sr = load_audiofile(validation_clip)\n",
    "    validation_data.append([get_melspectrogram(validation_audio), bird])\n",
    "\n",
    "# Split the single clips into training and validation\n",
    "random.shuffle(birds_with_single_clip)\n",
    "split_index = len(birds_with_single_clip) // 2\n",
    "training_single_clips = birds_with_single_clip[split_index:]\n",
    "validation_single_clips = birds_with_single_clip[:split_index]\n",
    "\n",
    "# Append the single clips to the training and validation data\n",
    "for bird in birds_with_single_clip:\n",
    "    clip = meta_data[meta_data['common_name'] == bird]['filename'].tolist()[0]\n",
    "    audio, sr = load_audiofile(clip)\n",
    "    split_index = len(audio) // 2\n",
    "    val_audio = audio[:split_index]\n",
    "    train_audio = audio[split_index:]\n",
    "    validation_data.append([get_melspectrogram(val_audio), bird])\n",
    "    training_data.append([get_melspectrogram(train_audio), bird])\n",
    "\n",
    "\n",
    "# Convert to numpy arrays\n",
    "training_data = np.asarray(training_data)\n",
    "validation_data = np.asarray(validation_data)\n",
    "\n",
    "# Map labels to indices\n",
    "label_mapping = {label: index for index, label in enumerate(set(meta_data['common_name'].unique()))}\n",
    "training_data[:, 1] = [label_mapping.get(label, -1) + 1 for label in training_data[:, 1]]\n",
    "validation_data[:, 1] = [label_mapping.get(label, -1) + 1 for label in validation_data[:, 1]]\n",
    "\n",
    "\n",
    "# Clear temporary data and variables\n",
    "birds_with_single_clip = None\n",
    "training_single_clips = None\n",
    "validation_single_clips = None\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From audiofiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(0, num_classes)  # Initialize with size 0\n",
    "        self.fc2 = nn.Linear(num_classes, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        size = x.shape[1] * x.shape[2] * x.shape[3]\n",
    "        x = x.view(x.size(0), -1)\n",
    "        self.fc1 = nn.Linear(size, num_classes)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Load data into batches of 32\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(training_data.tolist(), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(validation_data.tolist(), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the CNN and use +1 for classes due to \"no class\" being labeled as -1\n",
    "num_classes = len(meta_data['common_name'].unique())+1\n",
    "cnn = CNN(num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop and attempt to use cuda\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cnn.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch : \", epoch)\n",
    "    cnn.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    i = 0 \n",
    "    for images, labels in train_loader:\n",
    "        #load data onto device, either gpu or cpu\n",
    "        images = images.unsqueeze(1).to(device) \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        i+=1\n",
    "        \n",
    "    # Validation loop\n",
    "    cnn.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    \n",
    "    with torch.no_grad(): #Disables some calculations, used to reduce memory.\n",
    "        for images, labels in val_loader:\n",
    "        #load data onto device, either gpu or cpu\n",
    "            images = images.unsqueeze(1).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = cnn(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc = train_correct / len(train_loader.dataset)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = val_correct / len(val_loader.dataset)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(12288, num_classes)\n",
    "        self.fc2 = nn.Linear(num_classes, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Load data into batches of 32\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the CNN and use +1 for classes due to \"no class\" being labeled as -1\n",
    "num_classes = len(meta_data['common_name'].unique())+1\n",
    "cnn = CNN(num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop and attempt to use cuda\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cnn.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch : \", epoch)\n",
    "    cnn.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    i = 0 \n",
    "    for images, labels in train_loader:\n",
    "        #load data onto device, either gpu or cpu\n",
    "        images = images.unsqueeze(1).to(device) \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        i+=1\n",
    "        \n",
    "    # Validation loop\n",
    "    cnn.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    \n",
    "    with torch.no_grad(): #Disables some calculations, used to reduce memory.\n",
    "        for images, labels in val_loader:\n",
    "        #load data onto device, either gpu or cpu\n",
    "            images = images.unsqueeze(1).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = cnn(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc = train_correct / len(train_loader.dataset)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = val_correct / len(val_loader.dataset)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "APP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
