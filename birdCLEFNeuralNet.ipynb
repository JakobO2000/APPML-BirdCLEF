{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from birdCLEFloaddata import load_audiofile,load_metadata,get_melspectrogram\n",
    "from birdCLEFFunctions import Dynamic_CNN, Dynamic_CNN2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import Resize\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import soundfile as sf\n",
    "import random\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "path = r\"C:\\Users\\zhakk\\Desktop\\Uni\\Kandidat\\AML-BirdCLEFproject\\data\\birdCLEF2023\"\n",
    "\n",
    "meta_data = load_metadata(path,trim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audiofile2(filepath, cutoff_time):\n",
    "    #Function that takes a file and cutoff time to create training and validation sets for training\n",
    "    #If the audioclip is lower than the cutoff_time then the clip is looped untill desired duration reached\n",
    "    audio, sr = sf.read(filepath)\n",
    "    duration = len(audio) / sr\n",
    "\n",
    "\n",
    "    if duration >= cutoff_time:\n",
    "        training_audio = audio[:int(sr * 15)]\n",
    "        validation_audio = audio[int(sr * 15):int(sr * 30)]\n",
    "    else:\n",
    "        \n",
    "        loop_count = int(np.ceil(cutoff_time / duration))\n",
    "        audio = np.tile(audio, loop_count)\n",
    "\n",
    "        training_audio = audio[:int(sr * 15)]\n",
    "        validation_audio = audio[int(sr * 15):int(sr * 30)]\n",
    "\n",
    "    return training_audio.astype(np.float32), validation_audio.astype(np.float32), sr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First/simple data extraction methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate audio data\n",
    "training_data = []\n",
    "validation_data = []\n",
    "for i in range(50):\n",
    "    train_audio,validation_audio, sr = load_audiofile2(meta_data['filename'][i],30)\n",
    "    training_data.append([get_melspectrogram(train_audio),meta_data['common_name'][i]])\n",
    "    validation_data.append([get_melspectrogram(validation_audio),meta_data['common_name'][i]])\n",
    "\n",
    "\n",
    "training_data = np.asarray(training_data)\n",
    "validation_data = np.asarray(validation_data)\n",
    "label_mapping = {label: index for index, label in enumerate(set(meta_data['common_name'].unique()))}\n",
    "training_data[:,1] = [label_mapping.get(label, -1)+1 for label in training_data[:,1]]\n",
    "validation_data[:,1] = [label_mapping.get(label, -1)+1 for label in validation_data[:,1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Generate audio data\n",
    "training_data = []\n",
    "validation_data = []\n",
    "birds_with_single_clip = []\n",
    "\n",
    "# Iterate through each unique bird\n",
    "for bird in meta_data['common_name'].unique():\n",
    "    print(bird)\n",
    "    # Get all audio clips for the bird\n",
    "    bird_clips = meta_data[meta_data['common_name'] == bird]['filename'].tolist()\n",
    "    # If the bird has only one clip, split it into two halves\n",
    "    if len(bird_clips) == 1:\n",
    "        birds_with_single_clip.append(bird)\n",
    "    else:\n",
    "        # Randomly select one clip for validation and the rest for training\n",
    "        random.shuffle(bird_clips)\n",
    "        training_clip = bird_clips[1:]\n",
    "        validation_clip = bird_clips[0]\n",
    "    \n",
    "    # Load and process the training clip\n",
    "    for clip in training_clip:\n",
    "        train_audio, sr = load_audiofile(clip)\n",
    "        training_data.append([get_melspectrogram(train_audio), bird])\n",
    "    \n",
    "    # Load and process the validation clip\n",
    "    validation_audio, sr = load_audiofile(validation_clip)\n",
    "    validation_data.append([get_melspectrogram(validation_audio), bird])\n",
    "\n",
    "# Split the single clips into training and validation\n",
    "random.shuffle(birds_with_single_clip)\n",
    "split_index = len(birds_with_single_clip) // 2\n",
    "training_single_clips = birds_with_single_clip[split_index:]\n",
    "validation_single_clips = birds_with_single_clip[:split_index]\n",
    "\n",
    "# Append the single clips to the training and validation data\n",
    "for bird in birds_with_single_clip:\n",
    "    clip = meta_data[meta_data['common_name'] == bird]['filename'].tolist()[0]\n",
    "    audio, sr = load_audiofile(clip)\n",
    "    split_index = len(audio) // 2\n",
    "    val_audio = audio[:split_index]\n",
    "    train_audio = audio[split_index:]\n",
    "    validation_data.append([get_melspectrogram(val_audio), bird])\n",
    "    training_data.append([get_melspectrogram(train_audio), bird])\n",
    "\n",
    "\n",
    "# Convert to numpy arrays\n",
    "training_data = np.asarray(training_data)\n",
    "validation_data = np.asarray(validation_data)\n",
    "\n",
    "# Map labels to indices\n",
    "label_mapping = {label: index for index, label in enumerate(set(meta_data['common_name'].unique()))}\n",
    "training_data[:, 1] = [label_mapping.get(label, -1) + 1 for label in training_data[:, 1]]\n",
    "validation_data[:, 1] = [label_mapping.get(label, -1) + 1 for label in validation_data[:, 1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_audio.ndim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#Generate audio data\n",
    "training_data = []\n",
    "validation_data = []\n",
    "cutoff_time = 30\n",
    "\n",
    "for common_name in meta_data['common_name'].unique():\n",
    "    i = meta_data.loc[meta_data['common_name'] == common_name].index[0] \n",
    "    train_audio,validation_audio, sr = load_audiofile2(meta_data['filename'][i],cutoff_time)\n",
    "    training_data.append([get_melspectrogram(train_audio,n_fft=2048,hop_length=512,n_mels=128,fmin=40,fmax=15000,power=2.0,top_db=100),meta_data['common_name'][i]])\n",
    "    validation_data.append([get_melspectrogram(validation_audio,n_fft=2048,hop_length=512,n_mels=128,fmin=40,fmax=15000,power=2.0,top_db=100),meta_data['common_name'][i]])\n",
    "\n",
    "#Random selection of 100 data points, each point is taken in groups of 5\n",
    "selected_spots = random.sample(range(len(meta_data) - 4), 1)  \n",
    "\n",
    "for i in selected_spots:\n",
    "    for j in range(i, i + 4):\n",
    "        train_audio,validation_audio, sr = load_audiofile2(meta_data['filename'][i],cutoff_time)\n",
    "        training_data.append([get_melspectrogram(train_audio,n_fft=2048,hop_length=512,n_mels=128,fmin=40,fmax=15000,power=2.0,top_db=100),meta_data['common_name'][i]])\n",
    "        validation_data.append([get_melspectrogram(validation_audio,n_fft=2048,hop_length=512,n_mels=128,fmin=40,fmax=15000,power=2.0,top_db=100),meta_data['common_name'][i]])\n",
    "\n",
    "\n",
    "training_data = np.asarray(training_data)\n",
    "validation_data = np.asarray(validation_data)\n",
    "label_mapping = {label: index for index, label in enumerate(set(meta_data['common_name'].unique()))}\n",
    "training_data[:,1] = [label_mapping.get(label, -1)+1 for label in training_data[:,1]]\n",
    "validation_data[:,1] = [label_mapping.get(label, -1)+1 for label in validation_data[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "validation_data = []\n",
    "cutoff_time = 30\n",
    "\n",
    "for common_name in meta_data['common_name'].unique():\n",
    "    i = meta_data.loc[meta_data['common_name'] == common_name].index[0] \n",
    "    train_audio,validation_audio, sr = load_audiofile2(meta_data['filename'][i],cutoff_time)\n",
    "    training_data.append([get_melspectrogram(train_audio,n_fft=2048,hop_length=512,n_mels=128,fmin=40,fmax=15000,power=2.0,top_db=100),meta_data['common_name'][i]])\n",
    "    validation_data.append([get_melspectrogram(validation_audio,n_fft=2048,hop_length=512,n_mels=128,fmin=40,fmax=15000,power=2.0,top_db=100),meta_data['common_name'][i]])\n",
    "\n",
    "training_data = np.asarray(training_data)\n",
    "validation_data = np.asarray(validation_data)\n",
    "label_mapping = {label: index for index, label in enumerate(set(meta_data['common_name'].unique()))}\n",
    "training_data[:,1] = [label_mapping.get(label, -1)+1 for label in training_data[:,1]]\n",
    "validation_data[:,1] = [label_mapping.get(label, -1)+1 for label in validation_data[:,1]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectrogram extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate spectrograms\n",
    "spectrograms = []\n",
    "for i in range(50):\n",
    "    spectrograms.append([get_melspectrogram(meta_data['filename'][i]),meta_data['common_name'][i]])\n",
    "spectrograms = np.asarray(spectrograms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load one of each birds data\n",
    "spectrograms = []\n",
    "\n",
    "for common_name in meta_data['common_name'].unique():\n",
    "    index = meta_data.loc[meta_data['common_name'] == common_name].index[0] \n",
    "    spectrogram = get_melspectrogram(meta_data['filename'][index])  \n",
    "    spectrograms.append([spectrogram, common_name])\n",
    "\n",
    "spectrograms = np.asarray(spectrograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#Random selection of 100 data points, each point is taken in groups of 5\n",
    "spectrograms = []\n",
    "selected_spots = random.sample(range(len(meta_data) - 4), 100)  \n",
    "\n",
    "for i in selected_spots:\n",
    "    for j in range(i, i + 4):\n",
    "        spectrograms.append([get_melspectrogram(meta_data['filename'][j]), meta_data['common_name'][j]])\n",
    "\n",
    "spectrograms = np.asarray(spectrograms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate audio data\n",
    "training_data = []\n",
    "validation_data = []\n",
    "for i in range(50):\n",
    "    train_audio,validation_audio, sr = load_audiofile(meta_data['filename'][i],35)\n",
    "    training_data.append([get_melspectrogram(train_audio),meta_data['common_name'][i]])\n",
    "    validation_data.append([get_melspectrogram(validation_audio),meta_data['common_name'][i]])\n",
    "\n",
    "training_data = np.asarray(training_data)\n",
    "validation_data = np.asarray(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into validation and test. (Need to look at this again, possible mistake in the validation data creation)\n",
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * len(spectrograms))\n",
    "\n",
    "train_labels = spectrograms[:,1]\n",
    "\n",
    "label_mapping = {label: index for index, label in enumerate(set(train_labels))}\n",
    "\n",
    "spectrograms[:,1] = [label_mapping.get(label, -1)+1 for label in train_labels]\n",
    "\n",
    "validation_set = []\n",
    "training_set = []\n",
    "\n",
    "#Split data \n",
    "for i, (spectrogram, label) in enumerate(spectrograms):\n",
    "    shape = np.shape(spectrogram)\n",
    "    if shape[1] >= 100:\n",
    "        validation_data = spectrogram[:, :50]\n",
    "        validation_set.append([validation_data, label])\n",
    "        \n",
    "        remaining_data = spectrogram[:, 50:]\n",
    "        num_chunks = remaining_data.shape[1] // 50\n",
    "        if num_chunks > 0:\n",
    "            chunks = np.split(remaining_data[:, :num_chunks*50], num_chunks, axis=1)\n",
    "            for chunk in chunks:\n",
    "                training_set.append([chunk, label])\n",
    "    else: print(i,label)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 264\n",
      "1 / 264\n",
      "2 / 264\n",
      "3 / 264\n",
      "4 / 264\n",
      "5 / 264\n",
      "6 / 264\n",
      "7 / 264\n",
      "8 / 264\n",
      "9 / 264\n",
      "10 / 264\n",
      "11 / 264\n",
      "12 / 264\n",
      "13 / 264\n",
      "14 / 264\n",
      "15 / 264\n",
      "16 / 264\n",
      "17 / 264\n",
      "18 / 264\n",
      "19 / 264\n",
      "20 / 264\n",
      "21 / 264\n",
      "22 / 264\n",
      "23 / 264\n",
      "24 / 264\n",
      "25 / 264\n",
      "26 / 264\n",
      "27 / 264\n",
      "28 / 264\n",
      "29 / 264\n",
      "30 / 264\n",
      "31 / 264\n",
      "32 / 264\n",
      "33 / 264\n",
      "34 / 264\n",
      "35 / 264\n",
      "36 / 264\n",
      "37 / 264\n",
      "38 / 264\n",
      "39 / 264\n",
      "40 / 264\n",
      "41 / 264\n",
      "42 / 264\n",
      "43 / 264\n",
      "44 / 264\n",
      "45 / 264\n",
      "46 / 264\n",
      "47 / 264\n",
      "48 / 264\n",
      "49 / 264\n",
      "50 / 264\n",
      "51 / 264\n",
      "52 / 264\n",
      "53 / 264\n",
      "54 / 264\n",
      "55 / 264\n",
      "56 / 264\n",
      "57 / 264\n",
      "58 / 264\n",
      "59 / 264\n",
      "60 / 264\n",
      "61 / 264\n",
      "62 / 264\n",
      "63 / 264\n",
      "64 / 264\n",
      "65 / 264\n",
      "66 / 264\n",
      "67 / 264\n",
      "68 / 264\n",
      "69 / 264\n",
      "70 / 264\n",
      "71 / 264\n",
      "72 / 264\n",
      "73 / 264\n",
      "74 / 264\n",
      "75 / 264\n",
      "76 / 264\n",
      "77 / 264\n",
      "78 / 264\n",
      "79 / 264\n",
      "80 / 264\n",
      "81 / 264\n",
      "82 / 264\n",
      "83 / 264\n",
      "84 / 264\n",
      "85 / 264\n",
      "86 / 264\n",
      "87 / 264\n",
      "88 / 264\n",
      "89 / 264\n",
      "90 / 264\n",
      "91 / 264\n",
      "92 / 264\n",
      "93 / 264\n",
      "94 / 264\n",
      "95 / 264\n",
      "96 / 264\n",
      "97 / 264\n",
      "98 / 264\n",
      "99 / 264\n",
      "100 / 264\n",
      "101 / 264\n",
      "102 / 264\n",
      "103 / 264\n",
      "104 / 264\n",
      "105 / 264\n",
      "106 / 264\n",
      "107 / 264\n",
      "108 / 264\n",
      "109 / 264\n",
      "110 / 264\n",
      "111 / 264\n",
      "112 / 264\n",
      "113 / 264\n",
      "114 / 264\n",
      "115 / 264\n",
      "116 / 264\n",
      "117 / 264\n",
      "118 / 264\n",
      "119 / 264\n",
      "120 / 264\n",
      "121 / 264\n",
      "122 / 264\n",
      "123 / 264\n",
      "124 / 264\n",
      "125 / 264\n",
      "126 / 264\n",
      "127 / 264\n",
      "128 / 264\n",
      "129 / 264\n",
      "130 / 264\n",
      "131 / 264\n",
      "132 / 264\n",
      "133 / 264\n",
      "134 / 264\n",
      "135 / 264\n",
      "136 / 264\n",
      "137 / 264\n",
      "138 / 264\n",
      "139 / 264\n",
      "140 / 264\n",
      "141 / 264\n",
      "142 / 264\n",
      "143 / 264\n",
      "144 / 264\n",
      "145 / 264\n",
      "146 / 264\n",
      "147 / 264\n",
      "148 / 264\n",
      "149 / 264\n",
      "150 / 264\n",
      "151 / 264\n",
      "152 / 264\n",
      "153 / 264\n",
      "154 / 264\n",
      "155 / 264\n",
      "156 / 264\n",
      "157 / 264\n",
      "158 / 264\n",
      "159 / 264\n",
      "160 / 264\n",
      "161 / 264\n",
      "162 / 264\n",
      "163 / 264\n",
      "164 / 264\n",
      "165 / 264\n",
      "166 / 264\n",
      "167 / 264\n",
      "168 / 264\n",
      "169 / 264\n",
      "170 / 264\n",
      "171 / 264\n",
      "172 / 264\n",
      "173 / 264\n",
      "174 / 264\n",
      "175 / 264\n",
      "176 / 264\n",
      "177 / 264\n",
      "178 / 264\n",
      "179 / 264\n",
      "180 / 264\n",
      "181 / 264\n",
      "182 / 264\n",
      "183 / 264\n",
      "184 / 264\n",
      "185 / 264\n",
      "186 / 264\n",
      "187 / 264\n",
      "188 / 264\n",
      "189 / 264\n",
      "190 / 264\n",
      "191 / 264\n",
      "192 / 264\n",
      "193 / 264\n",
      "194 / 264\n",
      "195 / 264\n",
      "196 / 264\n",
      "197 / 264\n",
      "198 / 264\n",
      "199 / 264\n",
      "200 / 264\n",
      "201 / 264\n",
      "202 / 264\n",
      "203 / 264\n",
      "204 / 264\n",
      "205 / 264\n",
      "206 / 264\n",
      "207 / 264\n",
      "208 / 264\n",
      "209 / 264\n",
      "210 / 264\n",
      "211 / 264\n",
      "212 / 264\n",
      "213 / 264\n",
      "214 / 264\n",
      "215 / 264\n",
      "216 / 264\n",
      "217 / 264\n",
      "218 / 264\n",
      "219 / 264\n",
      "220 / 264\n",
      "221 / 264\n",
      "222 / 264\n",
      "223 / 264\n",
      "224 / 264\n",
      "225 / 264\n",
      "226 / 264\n",
      "227 / 264\n",
      "228 / 264\n",
      "229 / 264\n",
      "230 / 264\n",
      "231 / 264\n",
      "232 / 264\n",
      "233 / 264\n",
      "234 / 264\n",
      "235 / 264\n",
      "236 / 264\n",
      "237 / 264\n",
      "238 / 264\n",
      "239 / 264\n",
      "240 / 264\n",
      "241 / 264\n",
      "242 / 264\n",
      "243 / 264\n",
      "244 / 264\n",
      "245 / 264\n",
      "246 / 264\n",
      "247 / 264\n",
      "248 / 264\n",
      "249 / 264\n",
      "250 / 264\n",
      "251 / 264\n",
      "252 / 264\n",
      "253 / 264\n",
      "254 / 264\n",
      "255 / 264\n",
      "256 / 264\n",
      "257 / 264\n",
      "258 / 264\n",
      "259 / 264\n",
      "260 / 264\n",
      "261 / 264\n",
      "262 / 264\n",
      "263 / 264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhakk\\AppData\\Local\\Temp\\ipykernel_25224\\874054975.py:70: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  training_data = np.asarray(training_data)\n",
      "C:\\Users\\zhakk\\AppData\\Local\\Temp\\ipykernel_25224\\874054975.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training_data = np.asarray(training_data)\n",
      "C:\\Users\\zhakk\\AppData\\Local\\Temp\\ipykernel_25224\\874054975.py:71: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  validation_data = np.asarray(validation_data)\n",
      "C:\\Users\\zhakk\\AppData\\Local\\Temp\\ipykernel_25224\\874054975.py:71: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  validation_data = np.asarray(validation_data)\n"
     ]
    }
   ],
   "source": [
    "# Define the duration of each segment in seconds\n",
    "segment_duration = 5\n",
    "max_files_per_bird = 5\n",
    "\n",
    "# Generate audio data\n",
    "training_data = []\n",
    "validation_data = []\n",
    "birds_with_single_clip = []\n",
    "\n",
    "# Iterate through each unique bird\n",
    "for i, bird in enumerate(meta_data['common_name'].unique()):\n",
    "    print(i, \"/\", len(meta_data['common_name'].unique()))\n",
    "    # Get all audio clips for the bird\n",
    "    bird_clips = meta_data[meta_data['common_name'] == bird]['filename'].tolist()\n",
    "    # If the bird has only one clip, split it into two halves\n",
    "    if len(bird_clips) == 1:\n",
    "        birds_with_single_clip.append(bird)\n",
    "    else:\n",
    "        # Randomly select one clip for validation and the rest for training\n",
    "        random.shuffle(bird_clips)\n",
    "        training_clip = bird_clips[1:]\n",
    "        validation_clip = bird_clips[0]\n",
    "\n",
    "    # Load and process the training clip\n",
    "    num_files_sampled = 0\n",
    "    for clip in training_clip:\n",
    "        if num_files_sampled >= max_files_per_bird:\n",
    "            break\n",
    "        train_audio, sr = load_audiofile(clip)\n",
    "        num_segments = math.floor(len(train_audio) / (segment_duration * sr))\n",
    "        for segment in range(num_segments):\n",
    "            start_time = segment * segment_duration\n",
    "            end_time = start_time + segment_duration\n",
    "            segment_audio = train_audio[start_time * sr:end_time * sr]\n",
    "            training_data.append([get_melspectrogram(segment_audio), bird])\n",
    "        num_files_sampled += 1\n",
    "\n",
    "    # Load and process the validation clip\n",
    "    validation_audio, sr = load_audiofile(validation_clip)\n",
    "    num_segments = math.floor(len(validation_audio) / (segment_duration * sr))\n",
    "    for segment in range(num_segments):\n",
    "        start_time = segment * segment_duration\n",
    "        end_time = start_time + segment_duration\n",
    "        segment_audio = validation_audio[start_time * sr:end_time * sr]\n",
    "        validation_data.append([get_melspectrogram(segment_audio), bird])\n",
    "\n",
    "# Split the single clips into training and validation\n",
    "random.shuffle(birds_with_single_clip)\n",
    "split_index = len(birds_with_single_clip) // 2\n",
    "training_single_clips = birds_with_single_clip[split_index:]\n",
    "validation_single_clips = birds_with_single_clip[:split_index]\n",
    "\n",
    "# Append the single clips to the training and validation data\n",
    "for bird in birds_with_single_clip:\n",
    "    if len(training_data) >= max_files_per_bird:\n",
    "        break\n",
    "    clip = meta_data[meta_data['common_name'] == bird]['filename'].tolist()[0]\n",
    "    audio, sr = load_audiofile(clip)\n",
    "    num_segments = math.floor(len(audio) / (segment_duration * sr))\n",
    "    for segment in range(num_segments):\n",
    "        start_time = segment * segment_duration\n",
    "        end_time = start_time + segment_duration\n",
    "        segment_audio = audio[start_time * sr:end_time * sr]\n",
    "        validation_data.append([get_melspectrogram(segment_audio), bird])\n",
    "        training_data.append([get_melspectrogram(segment_audio), bird])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "training_data = np.asarray(training_data)\n",
    "validation_data = np.asarray(validation_data)\n",
    "\n",
    "# Map labels to indices\n",
    "label_mapping = {label: index for index, label in enumerate(set(meta_data['common_name'].unique()))}\n",
    "training_data[:, 1] = [label_mapping.get(label, -1) + 1 for label in training_data[:, 1]]\n",
    "validation_data[:, 1] = [label_mapping.get(label, -1) + 1 for label in validation_data[:, 1]]\n",
    "\n",
    "# Clear temporary data and variables\n",
    "birds_with_single_clip = None\n",
    "training_single_clips = None\n",
    "validation_single_clips = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7818, 1601)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data),len(validation_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From audiofiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(0, num_classes)  # Initialize with size 0\n",
    "        self.fc2 = nn.Linear(num_classes, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        size = x.shape[1] * x.shape[2] * x.shape[3]\n",
    "        x = x.view(x.size(0), -1)\n",
    "        self.fc1 = nn.Linear(size, num_classes)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Load data into batches of 32\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(training_data.tolist(), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(validation_data.tolist(), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the CNN and use +1 for classes due to \"no class\" being labeled as -1\n",
    "num_classes = len(meta_data['common_name'].unique())+1\n",
    "cnn = CNN(num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop and attempt to use cuda\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cnn.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch : \", epoch)\n",
    "    cnn.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    i = 0 \n",
    "    for images, labels in train_loader:\n",
    "        #load data onto device, either gpu or cpu\n",
    "        images = images.unsqueeze(1).to(device) \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        i+=1\n",
    "        \n",
    "    # Validation loop\n",
    "    cnn.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    \n",
    "    with torch.no_grad(): #Disables some calculations, used to reduce memory.\n",
    "        for images, labels in val_loader:\n",
    "        #load data onto device, either gpu or cpu\n",
    "            images = images.unsqueeze(1).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = cnn(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc = train_correct / len(train_loader.dataset)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = val_correct / len(val_loader.dataset)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(12288, num_classes)\n",
    "        self.fc2 = nn.Linear(num_classes, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Load data into batches of 32\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the CNN and use +1 for classes due to \"no class\" being labeled as -1\n",
    "num_classes = len(meta_data['common_name'].unique())+1\n",
    "cnn = CNN(num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop and attempt to use cuda\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cnn.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch : \", epoch)\n",
    "    cnn.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    i = 0 \n",
    "    for images, labels in train_loader:\n",
    "        #load data onto device, either gpu or cpu\n",
    "        images = images.unsqueeze(1).to(device) \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        i+=1\n",
    "        \n",
    "    # Validation loop\n",
    "    cnn.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    \n",
    "    with torch.no_grad(): #Disables some calculations, used to reduce memory.\n",
    "        for images, labels in val_loader:\n",
    "        #load data onto device, either gpu or cpu\n",
    "            images = images.unsqueeze(1).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = cnn(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc = train_correct / len(train_loader.dataset)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = val_correct / len(val_loader.dataset)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "APP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
